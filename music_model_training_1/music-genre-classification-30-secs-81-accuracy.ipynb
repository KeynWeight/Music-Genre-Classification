{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook uses xgboost to do a multilabel classification of music genre. This workbook uses GTZAN dataset for model training. The dataset consists of the original wav files of the songs,the image data of the songs, and the features data of the songs extracted in csv. In this notebook, we'll use the extracted features to build the model","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries and Load Dataset","metadata":{}},{"cell_type":"code","source":"# Install and update essential libraries and tools\n!pip install xgboost -U\n!pip install librosa -U\n# !apt-get --yes install ffmpeg\n!pip install hyperopt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-16T09:03:58.279567Z","iopub.execute_input":"2022-01-16T09:03:58.279966Z","iopub.status.idle":"2022-01-16T09:04:26.591201Z","shell.execute_reply.started":"2022-01-16T09:03:58.279919Z","shell.execute_reply":"2022-01-16T09:04:26.589627Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(\"ignore\", UserWarning)\n\nimport os\nfrom tqdm import tqdm\nimport pickle\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport librosa\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFECV,mutual_info_regression\nfrom sklearn.metrics import confusion_matrix, accuracy_score,classification_report\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.decomposition import PCA\n\nfrom xgboost import XGBClassifier\n\n#For hyperparameter tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:04:26.593345Z","iopub.execute_input":"2022-01-16T09:04:26.593716Z","iopub.status.idle":"2022-01-16T09:04:30.170664Z","shell.execute_reply.started":"2022-01-16T09:04:26.593678Z","shell.execute_reply":"2022-01-16T09:04:30.169038Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"There are 2 csv files of the features. One has the features for each song for 30 seconds long and another one just has the features of 3 seconds long. The 3-sec dataset contains data of 9990 songs whilst the 30-sec dataset only has 1000 songs (the songs in the genre folder). Looking at the other notebooks, they get a pretty good result making a model from a 3-sec dataset but just an average result for 30-sec dataset. This maybe to the lack of training data.\n\nDespite this, inuitively it is rather difficult to classify a song just by 3 seconds. Making a model from 3 seconds song may affect the real prediction power.\nTherefore, we will explore the 30-sec dataset and see what we can do to create a good model","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/gtzan-dataset-music-genre-classification/Data/features_30_sec.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:04:30.173673Z","iopub.execute_input":"2022-01-16T09:04:30.174003Z","iopub.status.idle":"2022-01-16T09:04:30.280717Z","shell.execute_reply.started":"2022-01-16T09:04:30.173963Z","shell.execute_reply":"2022-01-16T09:04:30.279813Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:04:30.282587Z","iopub.execute_input":"2022-01-16T09:04:30.282954Z","iopub.status.idle":"2022-01-16T09:04:30.300366Z","shell.execute_reply.started":"2022-01-16T09:04:30.282906Z","shell.execute_reply":"2022-01-16T09:04:30.299500Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:04:30.301576Z","iopub.execute_input":"2022-01-16T09:04:30.302180Z","iopub.status.idle":"2022-01-16T09:04:30.328190Z","shell.execute_reply.started":"2022-01-16T09:04:30.302142Z","shell.execute_reply":"2022-01-16T09:04:30.326866Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:04:30.329611Z","iopub.execute_input":"2022-01-16T09:04:30.329979Z","iopub.status.idle":"2022-01-16T09:04:30.468872Z","shell.execute_reply.started":"2022-01-16T09:04:30.329947Z","shell.execute_reply":"2022-01-16T09:04:30.467559Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Initial findings:\n1. Only 1000 songs are in the dataset with no null values\n2. `length` is not a relevant variable which needs to be dropped in model training phase\n3. The data are sorted in the same order as the songs in the folders of the dataset and the classes are balanced\n4. There are some other music features that this dataset did not extract, for example, spectral contrast, spectral flatness, tonal centroid features etc. Given it is just a small dataset, we can extract them and put it in the dataframe for further analysis","metadata":{}},{"cell_type":"markdown","source":"# Extracting extra features and data cleaning\nWe will now extract the mean and the variance of `chroma_cens`,`spectral_contrast`, `spectral_flatness` and `tonnetz`","metadata":{}},{"cell_type":"code","source":"songs_path = '../input/gtzan-dataset-music-genre-classification/Data/genres_original'\n\ndef extract_new_features(song_path, num_files = 1000, num_new_features = 8):\n\n    data_array = np.empty([num_files, num_new_features])\n\n    counter = 0\n    for root, dirs, files in os.walk(songs_path):\n        dirs.sort()\n        for file, i in zip(sorted(files), tqdm(range(num_files))):\n            i = i + (counter-1)*100\n            file_path = os.path.join(root, file)\n        \n            try:\n                #extract mean and variance of those 4 features\n                y, sr = librosa.load(os.fspath(file_path))\n                chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n                spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n                spectral_flatness = librosa.feature.spectral_flatness(y=y)\n                tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n\n                data_array[i,0] = np.mean(chroma_cens)\n                data_array[i,1] = np.var(chroma_cens)\n                data_array[i,2] = np.mean(spectral_contrast)\n                data_array[i,3] = np.var(spectral_contrast)\n                data_array[i,4] = np.mean(spectral_flatness)\n                data_array[i,5] = np.var(spectral_flatness)\n                data_array[i,6] = np.mean(tonnetz)\n                data_array[i,7] = np.var(tonnetz)\n                \n            # Set all values to zero for files with problems\n            except:\n                print(f'Problem file: {file_path}')\n                for j in range(num_new_features):\n                    data_array[i, j] = 0 \n                \n        counter += 1\n                  \n    return data_array\n\n\nnew_features_array = extract_new_features('../input/gtzan-dataset-music-genre-classification/Data/genres_original')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:04:30.470421Z","iopub.execute_input":"2022-01-16T09:04:30.470666Z","iopub.status.idle":"2022-01-16T09:29:51.430261Z","shell.execute_reply.started":"2022-01-16T09:04:30.470632Z","shell.execute_reply":"2022-01-16T09:29:51.428960Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"We can see that in the dataset, `jazz.00054.wav` this file has a problem and can't be played. We will initialize the values to 0 and will do further treatment after.","metadata":{}},{"cell_type":"code","source":"#Add those new features back to the original dataframe\n\ndf['chroma_cens_mean'] = new_features_array[:,0]\ndf['chroma_cens_var'] = new_features_array[:,1]\ndf['spectral_contrast_mean'] = new_features_array[:,2]\ndf['spectral_contrast_var'] = new_features_array[:,3]\ndf['spectral_flatness_mean'] = new_features_array[:,4]\ndf['spectral_flatness_var'] = new_features_array[:,5]\ndf['tonnetz_mean'] = new_features_array[:,6]\ndf['tonnetz_var'] = new_features_array[:,7]","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:29:51.437254Z","iopub.execute_input":"2022-01-16T09:29:51.439108Z","iopub.status.idle":"2022-01-16T09:29:51.456785Z","shell.execute_reply.started":"2022-01-16T09:29:51.438993Z","shell.execute_reply":"2022-01-16T09:29:51.455773Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Since there is only 1 row of missing value, now we'll just fill in the missing values of `jazz.0054.wav` as the mean of those of the jazz songs, which is at index 554","metadata":{}},{"cell_type":"code","source":"for i in range(-8,0,1):\n    # Filter out the jazz genre except jazz.0054\n    df.iloc[554,i] = df[ df.label == 'jazz'].iloc[np.r_[np.arange(0,54),np.arange(55,100)],i].mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:29:51.460555Z","iopub.execute_input":"2022-01-16T09:29:51.461144Z","iopub.status.idle":"2022-01-16T09:29:51.494606Z","shell.execute_reply.started":"2022-01-16T09:29:51.461096Z","shell.execute_reply":"2022-01-16T09:29:51.493592Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Save as a new csv\ndf.to_csv('new_csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:29:51.496474Z","iopub.execute_input":"2022-01-16T09:29:51.497317Z","iopub.status.idle":"2022-01-16T09:29:51.661694Z","shell.execute_reply.started":"2022-01-16T09:29:51.497258Z","shell.execute_reply":"2022-01-16T09:29:51.660693Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#Exclude filename and Length\ndf = df.iloc[:,2:]","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:48.388043Z","iopub.execute_input":"2022-01-16T09:31:48.388619Z","iopub.status.idle":"2022-01-16T09:31:48.394102Z","shell.execute_reply.started":"2022-01-16T09:31:48.388570Z","shell.execute_reply":"2022-01-16T09:31:48.393255Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# EDA and Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"**Correlations between mean variables**","metadata":{}},{"cell_type":"code","source":"corr = df.corr()\n\n#Create a mask for the heatmap\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize=(20, 20))\nsns.heatmap(corr, mask=mask, cmap=\"vlag\")\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:53.355863Z","iopub.execute_input":"2022-01-16T09:31:53.356206Z","iopub.status.idle":"2022-01-16T09:31:56.110693Z","shell.execute_reply.started":"2022-01-16T09:31:53.356169Z","shell.execute_reply":"2022-01-16T09:31:56.109644Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Most of the variables do not have a high correlation with other variables. Let's filter out the extremely highly correlated pairs and examine them.","metadata":{}},{"cell_type":"code","source":"sol = (corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n                  .stack()\n                  .sort_values(ascending=False))\nfor index, value in sol.items():\n    if (value > 0.75) or (value < -0.75):\n        print(index, value)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:32:53.219545Z","iopub.execute_input":"2022-01-16T09:32:53.219883Z","iopub.status.idle":"2022-01-16T09:32:53.242211Z","shell.execute_reply.started":"2022-01-16T09:32:53.219847Z","shell.execute_reply":"2022-01-16T09:32:53.241501Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"**Split to train data and test data**\n\nGiven the small number of training data, I set 90% as training data and 10% as testing data. For hyparameter tuning, given the small dataset, we will use the same train dataset to tune\nFor the split of data, I make sure every class has the same number of data to train and test.","metadata":{}},{"cell_type":"code","source":"y = df.label\nX = df\n\n#Use `label` to split data evenly and drop `label` column after split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=df.label, random_state=77)\nX_train.drop('label',axis=1,inplace=True)\nX_test.drop('label',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:33:09.384758Z","iopub.execute_input":"2022-01-16T09:33:09.385134Z","iopub.status.idle":"2022-01-16T09:33:09.402871Z","shell.execute_reply.started":"2022-01-16T09:33:09.385094Z","shell.execute_reply":"2022-01-16T09:33:09.401595Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"**Normalize the data**","metadata":{}},{"cell_type":"code","source":"sc = StandardScaler()\nX_train_scaled = sc.fit_transform(X_train)\nX_train = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\nX_test_scaled = sc.transform(X_test)\nX_test = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:33:11.319333Z","iopub.execute_input":"2022-01-16T09:33:11.319643Z","iopub.status.idle":"2022-01-16T09:33:11.334412Z","shell.execute_reply.started":"2022-01-16T09:33:11.319611Z","shell.execute_reply":"2022-01-16T09:33:11.333434Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Initial model fitting and recursive feature elimination\nNow we will fit our training data to xgboost classifier first, and then we'll do RFECV to check which variables can be eliminated","metadata":{}},{"cell_type":"code","source":"estimator = XGBClassifier(eval_metric='merror')\nrfecv = RFECV(estimator, step=1, cv=5,scoring='accuracy',verbose=1)\nrfecv.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:33:19.158559Z","iopub.execute_input":"2022-01-16T09:33:19.158894Z","iopub.status.idle":"2022-01-16T09:41:15.630558Z","shell.execute_reply.started":"2022-01-16T09:33:19.158862Z","shell.execute_reply":"2022-01-16T09:41:15.629617Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# See which features can be eliminated\nfeatures_drop_array = list(np.where(rfecv.support_ == False)[0])\nX_train.columns[features_drop_array]","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:41:15.632138Z","iopub.execute_input":"2022-01-16T09:41:15.632369Z","iopub.status.idle":"2022-01-16T09:41:15.639513Z","shell.execute_reply.started":"2022-01-16T09:41:15.632340Z","shell.execute_reply":"2022-01-16T09:41:15.638619Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"X_train.drop(X_train.columns[features_drop_array], axis=1, inplace=True)\nX_test.drop(X_test.columns[features_drop_array], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:41:45.255068Z","iopub.execute_input":"2022-01-16T09:41:45.256382Z","iopub.status.idle":"2022-01-16T09:41:45.265700Z","shell.execute_reply.started":"2022-01-16T09:41:45.256309Z","shell.execute_reply":"2022-01-16T09:41:45.264045Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Model training\n\nWe will just use XGBoost Classifier to classify","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier(n_estimators=1000)\nmodel.fit(X_train,y_train,eval_metric='merror')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:41:47.145068Z","iopub.execute_input":"2022-01-16T09:41:47.145956Z","iopub.status.idle":"2022-01-16T09:41:56.050915Z","shell.execute_reply.started":"2022-01-16T09:41:47.145901Z","shell.execute_reply":"2022-01-16T09:41:56.049952Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"y_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\ntarget_names = sorted(set(y))\n\nprint(f'Training accuracy: {accuracy_score(y_train,y_pred_train)}')\nprint(f'Training:\\n {classification_report(y_train, y_pred_train, labels=target_names)}')\nprint(f'Testing accuracy: {accuracy_score(y_test,y_pred_test)}')\nprint(f'Testing:\\n {classification_report(y_test, y_pred_test, labels=target_names)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:41:56.052616Z","iopub.execute_input":"2022-01-16T09:41:56.052837Z","iopub.status.idle":"2022-01-16T09:41:56.144059Z","shell.execute_reply.started":"2022-01-16T09:41:56.052809Z","shell.execute_reply":"2022-01-16T09:41:56.143193Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"#Confusion matrix of the test data\ncm = confusion_matrix(y_test, y_pred_test)\nplt.figure(figsize = (16, 9))\nsns.heatmap(cm,cmap=\"Blues\", annot=True, xticklabels = target_names, yticklabels = target_names )","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:41:56.145424Z","iopub.execute_input":"2022-01-16T09:41:56.146297Z","iopub.status.idle":"2022-01-16T09:41:56.909269Z","shell.execute_reply.started":"2022-01-16T09:41:56.146222Z","shell.execute_reply":"2022-01-16T09:41:56.908443Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"We may see that we have got 99% of accuracy for training data, but just 78% for the testing data. Obviously, we are overfitting our model. In the following, we will add regularization parameters and tune other parameters to reduce this problem.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter tuning\nIn the following, we'll use `hyperopt` library to help tuning the parameters. The parameters may vary for each run","metadata":{}},{"cell_type":"code","source":"space={\n    'n_estimators': hp.quniform('n_estimators', 0,3000,1),\n    'reg_lambda' : hp.quniform('reg_lambda', 0,500,1),\n    }\n\ndef objective(space):\n    clf=XGBClassifier(\n                    n_estimators =int(space['n_estimators']),\n                    reg_lambda = int(space['reg_lambda']),\n                    )\n    \n    evaluation = [( X_train, y_train), ( X_test, y_test)]\n    \n    clf.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    \n\n    pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, pred)\n    return {'loss': -accuracy, 'status': STATUS_OK }\n\n\ntrials = Trials()\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 100,\n                        trials = trials)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:19:51.859788Z","iopub.execute_input":"2022-01-16T10:19:51.860354Z","iopub.status.idle":"2022-01-16T10:33:24.016377Z","shell.execute_reply.started":"2022-01-16T10:19:51.860303Z","shell.execute_reply":"2022-01-16T10:33:24.015659Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"print(f\"best params: {best_hyperparams}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:33:24.020096Z","iopub.execute_input":"2022-01-16T10:33:24.020770Z","iopub.status.idle":"2022-01-16T10:33:24.025016Z","shell.execute_reply.started":"2022-01-16T10:33:24.020731Z","shell.execute_reply":"2022-01-16T10:33:24.024292Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"model1 = XGBClassifier(n_estimators=304, reg_lambda=25)\nmodel1.fit(X_train,y_train,eval_metric='merror')\ny_pred_test1 = model1.predict(X_test)\nprint(f\"accuracy: {accuracy_score(y_test,y_pred_test1)}\")\nprint(f'New tuned model:\\n {classification_report(y_test, y_pred_test1, labels=target_names)}')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:35:58.121473Z","iopub.execute_input":"2022-01-16T10:35:58.121896Z","iopub.status.idle":"2022-01-16T10:36:07.652762Z","shell.execute_reply.started":"2022-01-16T10:35:58.121849Z","shell.execute_reply":"2022-01-16T10:36:07.652127Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"Our accuracy, precision and recall both improved a little bit from 78% to 81%. We will make this our final model","metadata":{}},{"cell_type":"markdown","source":"# Save the model and the preprocessing","metadata":{}},{"cell_type":"code","source":"pickle.dump(sc, open('sc.pkl','wb'))\npickle.dump(model1, open('model.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-01-16T10:36:31.492054Z","iopub.execute_input":"2022-01-16T10:36:31.492476Z","iopub.status.idle":"2022-01-16T10:36:31.708533Z","shell.execute_reply.started":"2022-01-16T10:36:31.492436Z","shell.execute_reply":"2022-01-16T10:36:31.707529Z"},"trusted":true},"execution_count":53,"outputs":[]}]}